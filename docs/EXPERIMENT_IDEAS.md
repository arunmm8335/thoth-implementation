# Thoth PCB Coalescing - Experiment Ideas

## Current System Status
Your implementation is **100% functional** with:
- ‚úÖ PCB coalescing (97.25% efficiency)
- ‚úÖ PLUB overflow path
- ‚úÖ 10ms ADR flush timing
- ‚úÖ Three performance formulas working
- ‚úÖ NVMain PCM backend (150ns/500ns)
- ‚úÖ Complete statistics collection

## üî¨ Experiments You Can Run Immediately

### 1. **Traffic Pattern Analysis** (Easy - 10 minutes)
**Goal:** See how different write patterns affect coalescing efficiency

**What to change in `thoth_full_demo.py`:**
```python
# Current: 255 requests, stride 8B
traffic_gen.max_requests = 255
traffic_gen.stride = 8

# Try these variations:
# Sequential writes (best coalescing):
traffic_gen.stride = 8
traffic_gen.max_requests = 512  # More data

# Random writes (worst coalescing):
traffic_gen.stride = 128  # Jump around in memory
traffic_gen.max_requests = 255

# Medium coalescing:
traffic_gen.stride = 16  # Skip every other 8B slot
```

**Expected Results:**
- Small stride (8B) ‚Üí High efficiency (97%+)
- Large stride (128B+) ‚Üí Low efficiency (<50%)
- Can plot: Stride vs. Coalescing Efficiency

---

### 2. **PCB Capacity Stress Test** (Easy - 15 minutes)
**Goal:** Force PLUB overflow to see overflow handling

**What to change:**
```python
# In metadata_cache.cc, line ~320:
// Current: if (pcbMap.size() >= 256)
// Change to smaller capacity:
if (pcbMap.size() >= 64)  // Force overflow sooner

# In thoth_full_demo.py:
traffic_gen.stride = 64  # Each write goes to different PCB entry
traffic_gen.max_requests = 1000  # Lots of unique addresses
```

**Expected Results:**
- PLUB overflows > 0
- PLUB Overhead formula shows non-zero value
- Can measure: Overflow Rate vs. PCB Size

---

### 3. **Flush Timing Sensitivity** (Medium - 20 minutes)
**Goal:** How does flush interval affect performance?

**What to change in `metadata_cache.cc`:**
```cpp
// Line ~370 in flushPCB():
// Current: schedule(flushEvent, curTick() + 10_ms);

// Try variations:
schedule(flushEvent, curTick() + 1_ms);   // Aggressive
schedule(flushEvent, curTick() + 5_ms);   // Moderate
schedule(flushEvent, curTick() + 20_ms);  // Lazy
schedule(flushEvent, curTick() + 100_ms); // Very lazy
```

**Expected Results:**
- Shorter flush ‚Üí More partial blocks flushed
- Longer flush ‚Üí Better coalescing efficiency
- Trade-off: Latency vs. Write Amplification
- Can plot: Flush Interval vs. Write Amp

---

### 4. **Write Amplification Analysis** (Easy - 10 minutes)
**Goal:** Measure write amplification under different scenarios

**Current result:** 0.251 (near-optimal 0.25)

**Experiments:**
```python
# A) Perfect coalescing (all 8B writes to same block):
traffic_gen.base_addr = '4GB'
traffic_gen.stride = 8
traffic_gen.max_requests = 512  # 512 * 8B = 4096B = 64 blocks
# Expected WA: 0.125 (64 NVM writes / 512 partials)

# B) No coalescing (all random addresses):
traffic_gen.stride = 64
traffic_gen.max_requests = 1000
# Expected WA: 1.0 (each partial becomes one NVM write)

# C) 50% coalescing:
traffic_gen.stride = 16  # Some overlap, some don't
# Expected WA: ~0.5-0.6
```

**Can generate paper figure:** Write Amplification vs. Access Pattern

---

### 5. **NVM Write Reduction** (Medium - 20 minutes)
**Goal:** Quantify how much traffic PCB saves vs. no coalescing

**Setup:**
1. Run WITH PCB (current system)
2. Create "no-coalescing" baseline:
   ```cpp
   // In coalescePartial(), bypass all logic:
   void MetadataCache::coalescePartial(PacketPtr pkt) {
       sendToPLUB(pkt);  // All partials go directly to PLUB
       return;
   }
   ```

**Metrics to compare:**
- NVM writes (should be 8√ó higher without PCB)
- NVM bytes written
- Write amplification
- Can create table: "PCB vs. No-PCB Comparison"

---

### 6. **Scalability Test** (Hard - 30 minutes)
**Goal:** See how system scales with more traffic

**What to change:**
```python
# Increase simulation time:
root.sim_quantum = Latency('100ms')  # Was 10ms

# More aggressive traffic:
traffic_gen.rate = '100GB/s'  # Was 40GB/s
traffic_gen.max_requests = 10000  # More requests

# Increase NVM capacity:
range=AddrRange('8GB', size='8GB')  # More space
```

**Expected Results:**
- PCB fills up more often
- PLUB overflows increase
- Can measure: Throughput vs. Coalescing Efficiency

---

## üéØ Research Questions You Can Answer

### For a Paper/Thesis:
1. **Q:** How does write locality affect coalescing efficiency?
   - **Experiment:** Vary stride from 8B to 1KB, measure efficiency
   
2. **Q:** What's the optimal PCB size?
   - **Experiment:** Test sizes 32, 64, 128, 256, 512 entries
   
3. **Q:** Is 10ms flush interval optimal?
   - **Experiment:** Test 1ms, 5ms, 10ms, 20ms, 50ms
   
4. **Q:** How much does PCB reduce NVM wear?
   - **Experiment:** Compare writes WITH vs. WITHOUT coalescing

5. **Q:** What's the overflow rate in realistic workloads?
   - **Experiment:** Trace-driven simulation (if you have memory traces)

---

## üìä Data You Can Collect

All statistics available in `m5out/stats.txt`:

### Coalescing Metrics:
- `pcbTotalPartials` - Total 8B writes received
- `pcbCoalescedBlocks` - Full 64B blocks created
- `pcbPartialFlushes` - Incomplete blocks flushed
- **Efficiency:** (Coalesced / Total) √ó 100

### Overflow Metrics:
- `pcbOverflows` - Times PCB was full
- `plubPartials` - Partials sent to PLUB
- `overflowRate` - Formula: (Overflows / Total) √ó 100
- `plubOverhead` - Formula: (PLUB / Total) √ó 100

### NVM Metrics:
- `nvmWrites` - Total writes to NVM
- `nvmBytesWritten` - Total bytes to NVM
- `writeAmplification` - Formula: NVM_writes / Expected_writes

### Performance Metrics:
- `cacheHits` / `cacheMisses` - Cache hit rate
- `totalLatency` - Average latency

---

## üöÄ Quick Start: Run Your First Experiment

**Experiment:** "How does traffic rate affect coalescing?"

```bash
# 1. Baseline (current)
./build/RISCV/gem5.opt configs/example/thoth_full_demo.py
grep "pcbCoalescedBlocks\|writeAmplification" m5out/stats.txt > results_40gbps.txt

# 2. Slower traffic
# Edit thoth_full_demo.py: traffic_gen.rate = '10GB/s'
./build/RISCV/gem5.opt configs/example/thoth_full_demo.py
grep "pcbCoalescedBlocks\|writeAmplification" m5out/stats.txt > results_10gbps.txt

# 3. Faster traffic
# Edit thoth_full_demo.py: traffic_gen.rate = '100GB/s'
./build/RISCV/gem5.opt configs/example/thoth_full_demo.py
grep "pcbCoalescedBlocks\|writeAmplification" m5out/stats.txt > results_100gbps.txt

# 4. Compare
echo "=== Traffic Rate vs. Coalescing Efficiency ==="
cat results_*gbps.txt
```

---

## üìà Suggested Figures for Paper

1. **Figure 1:** PCB Coalescing Efficiency vs. Write Stride
   - X-axis: Stride (8B, 16B, 32B, 64B, 128B)
   - Y-axis: Coalescing Efficiency (%)
   
2. **Figure 2:** Write Amplification vs. PCB Size
   - X-axis: PCB Entries (32, 64, 128, 256, 512)
   - Y-axis: Write Amplification Factor
   
3. **Figure 3:** NVM Traffic Reduction
   - Bar chart: With PCB vs. Without PCB
   - Y-axis: NVM Writes (log scale)
   
4. **Figure 4:** PLUB Overflow Rate vs. Traffic Intensity
   - X-axis: Traffic Rate (GB/s)
   - Y-axis: Overflow Rate (%)

---

## ‚ö†Ô∏è What You CANNOT Do Yet

These require the optional TODO items:

1. ‚ùå **Stale Block Analysis** - No threshold logic implemented
2. ‚ùå **6HB Block Size** - Need to research what 6HB means
3. ‚ùå **1TB NVM Scale** - Would be very slow (can change if needed)

But honestly, you don't need these for experiments! Your core system works great.

---

## üí° Bottom Line

**YES, you can absolutely do experiments!**

Your implementation is publication-ready for:
- Conference papers (AES, ISPASS, etc.)
- Master's thesis
- Research reports
- Performance analysis studies

The system works, collects all metrics, and can answer real research questions.

**Start with Experiment #1 (Traffic Pattern Analysis) - takes 10 minutes!**
